/**
 * Hash Stability Integration Tests
 *
 * Validates cross-cutting hash properties specified in the narrative layer spec:
 * - Timestamp independence (spec :5204-5266)
 * - Content sensitivity
 * - Key order independence (stableStringify determinism)
 * - Array reordering canonicalization
 * - Null/undefined handling consistency
 * - SHA-256 output format (64-char hex)
 *
 * These are integration-level tests that exercise the full pipeline:
 * canonicalizeEvidence -> stableStringify -> SHA-256 -> hex digest.
 *
 * @story US-NL01
 * @see docs/specs/narrative-layer-spec.md :5204-5266, :2144-2160
 */

import {
  computeNarrativeHash,
  computeSourceEvidenceHash,
  computeIntegrityHash,
  stableStringify,
  canonicalizeEvidence,
} from '@/lib/narrative/hash';
import type {
  PitchNarrativeContent,
  ValidationEvidence,
} from '@/lib/narrative/types';

// --- Inline Test Data Factories ---

function makeNarrativeContent(overrides?: Partial<PitchNarrativeContent>): PitchNarrativeContent {
  return {
    version: '1.0',
    cover: {
      venture_name: 'HashTest Ventures',
      tagline: 'Deterministic by design',
      document_type: 'Pitch Deck',
      presentation_date: '2026-02-05',
      contact: { founder_name: 'Alice', email: 'alice@example.com' },
    },
    overview: {
      thesis: 'We validate hashing stability.',
      one_liner: 'We hash X for Y by Z.',
      industry: 'Developer Tools',
      novel_insight: 'Determinism matters.',
      key_metrics: [{ label: 'Tests', value: '100%', evidence_type: 'DO-direct' }],
    },
    opportunity: {
      tam: { value: 1000000, unit: 'USD', timeframe: 'annual', source: 'research', confidence: 'estimated' },
      sam: { value: 500000, unit: 'USD', timeframe: 'annual', source: 'research', confidence: 'estimated' },
      som: { value: 100000, unit: 'USD', timeframe: 'annual', source: 'research', confidence: 'estimated' },
      growth_trajectory: 'Accelerating',
      why_now: 'Growing need for data integrity',
      market_tailwinds: ['Open source adoption'],
    },
    problem: {
      primary_pain: 'Hash instability',
      pain_narrative: 'Non-deterministic serialization causes false mismatches.',
      affected_population: '50,000 developers',
      why_exists: 'JSON key ordering is not guaranteed',
      status_quo: 'Ad-hoc string comparison',
      severity_score: 0.85,
      evidence_quotes: ['It broke in production'],
    },
    solution: {
      value_proposition: 'Canonical hashing',
      how_it_works: 'Sort keys, canonicalize arrays, then SHA-256.',
      key_differentiator: 'Cross-platform reproducibility',
      use_cases: ['Evidence integrity', 'Export verification'],
      fit_score: 0.78,
    },
    traction: {
      evidence_summary: 'Hash stability verified across 6 dimensions.',
      growth_metrics: [],
      assumptions_validated: [],
      do_direct: [],
      do_indirect: [],
      say_evidence: [],
      interview_count: 3,
      experiment_count: 1,
      hitl_completion_rate: 1.0,
      display_config: {
        evidence_order: ['do_direct', 'do_indirect', 'say_evidence'],
        show_weights: true,
        visual_emphasis: { do_direct: 'primary', do_indirect: 'secondary', say_evidence: 'tertiary' },
      },
    },
    customer: {
      segments: [],
      persona_summary: 'Quality-focused engineers.',
      demographics: { location: 'Global', behaviors: 'CI/CD heavy' },
      willingness_to_pay: '$30/mo',
      market_size: 50000,
      target_percentage: 0.05,
      target_first: 'Platform teams',
      acquisition_channel: 'Developer communities',
      behavioral_insights: [],
      segment_prioritization: 'Primary: platform teams',
    },
    competition: {
      landscape_summary: 'Few competitors in canonical hashing.',
      primary_competitors: [],
      secondary_competitors: [],
      differentiators: ['Cross-language parity'],
      unfair_advantage: 'Spec-driven approach',
      incumbent_defense: 'Network effects',
    },
    business_model: {
      revenue_model: 'SaaS',
      cac: 50,
      ltv: 600,
      ltv_cac_ratio: 12,
      unit_economics: {
        cost_per_unit: 5,
        revenue_per_unit: 30,
        margin_per_unit: 25,
        breakdown: [{ category: 'compute', amount: 5 }],
      },
      pricing_strategy: 'Usage-based',
      market_context: 'Growing DevOps market',
    },
    team: {
      members: [{
        name: 'Alice',
        current_role: 'CTO',
        bio: 'Hashing expert.',
        prior_experience: ['CryptoLib'],
        accomplishments: ['SHA-3 contributor'],
        domain_expertise: 'Cryptography',
      }],
      coachability_score: 0.9,
    },
    use_of_funds: {
      ask_amount: 250000,
      ask_type: 'SAFE',
      allocations: [{ category: 'Engineering', amount: 150000, percentage: 60 }],
      milestones: [{ description: 'v1 release', target_date: '2026-06-01', success_criteria: '1000 users' }],
      timeline_weeks: 20,
    },
    metadata: {
      methodology: 'VPD',
      evidence_strength: 'DO-direct',
      overall_fit_score: 0.78,
      validation_stage: 'feasibility',
      pivot_count: 0,
    },
    ...overrides,
  };
}

function makeValidationEvidence(overrides?: Partial<ValidationEvidence>): ValidationEvidence {
  return {
    vpc: {
      customer_segment: 'Platform Engineers',
      customer_jobs: ['Ensure data integrity', 'Automate verification'],
      pains: [
        { description: 'Hash mismatches', severity: 0.9 },
        { description: 'Key ordering bugs', severity: 0.7 },
      ],
      gains: [
        { description: 'Deterministic output', importance: 0.95 },
        { description: 'Cross-platform parity', importance: 0.8 },
      ],
      pain_relievers: ['Canonical serialization', 'Array sorting'],
      gain_creators: ['SHA-256 hex output', 'Spec compliance'],
      products_services: ['Hash SDK'],
      fit_assessment: 'Strong fit',
    },
    customer_profile: {
      segment_name: 'Platform Engineers',
      jobs_to_be_done: [
        { job: 'Verify exports', importance: 0.9, frequency: 'daily' },
        { job: 'Detect tampering', importance: 0.85, frequency: 'weekly' },
      ],
      pains: [
        { pain: 'Non-deterministic JSON', severity: 0.8, current_solution: 'Custom scripts' },
        { pain: 'Slow verification', severity: 0.6, current_solution: 'Manual review' },
      ],
      gains: [
        { gain: 'Instant verification', relevance: 0.9 },
        { gain: 'Audit trail', relevance: 0.75 },
      ],
      behavioral_insights: ['Prefer deterministic tools', 'Value reproducibility'],
    },
    competitor_map: {
      competitors: [
        { name: 'HashiCorp Vault', category: 'indirect', strengths: ['Ecosystem'], weaknesses: ['Complexity'] },
        { name: 'JCS (JSON Canonicalization)', category: 'direct', strengths: ['RFC'], weaknesses: ['Adoption'] },
      ],
      positioning_statement: 'Simple, spec-driven evidence hashing.',
      differentiation_axes: [],
    },
    bmc: {
      key_partners: ['Cloud providers', 'OSS foundations'],
      key_activities: ['SDK development', 'Spec maintenance'],
      key_resources: ['Engineering team', 'Spec documents'],
      value_propositions: ['Determinism', 'Speed', 'Simplicity'],
      customer_relationships: ['Developer docs', 'Community support'],
      channels: ['npm', 'PyPI'],
      customer_segments: ['Platform teams', 'Security teams'],
      cost_structure: [{ item: 'Engineering', type: 'fixed' }],
      revenue_streams: [{ stream: 'Enterprise license', type: 'recurring', pricing_model: 'per-seat' }],
    },
    experiment_results: [
      {
        experiment_id: 'exp-hash-01',
        hypothesis_id: 'hyp-determinism',
        experiment_type: 'prototype',
        start_date: '2026-01-01',
        end_date: '2026-01-10',
        sample_size: 50,
        success_criteria: '100% hash match rate',
        actual_result: '100% match across 50 inputs',
        outcome: 'validated',
        learnings: ['Sorting before hashing is essential'],
        evidence_category: 'DO-direct',
      },
      {
        experiment_id: 'exp-hash-02',
        hypothesis_id: 'hyp-performance',
        experiment_type: 'prototype',
        start_date: '2026-01-11',
        end_date: '2026-01-20',
        sample_size: 1000,
        success_criteria: '<10ms per hash',
        actual_result: '2ms average',
        outcome: 'validated',
        learnings: ['SHA-256 is fast enough'],
        evidence_category: 'DO-direct',
      },
    ],
    gate_scores: {
      desirability: 0.8,
      feasibility: 0.9,
      viability: 0.7,
      overall_fit: 0.78,
      current_gate: 'viability',
    },
    hitl_record: {
      checkpoints: [
        {
          checkpoint_id: 'cp-hash-1',
          checkpoint_type: 'desirability_review',
          triggered_at: '2026-01-15T09:00:00Z',
          responded_at: '2026-01-15T11:00:00Z',
          approval_status: 'approved',
        },
        {
          checkpoint_id: 'cp-hash-2',
          checkpoint_type: 'feasibility_review',
          triggered_at: '2026-01-25T09:00:00Z',
          responded_at: '2026-01-25T15:00:00Z',
          approval_status: 'approved',
        },
      ],
      coachability_score: 0.9,
      total_checkpoints: 2,
      completed_checkpoints: 2,
    },
    ...overrides,
  };
}

function makeIntegrity() {
  return {
    methodology_version: 'VPD-1.0',
    agent_versions: [
      { agent_name: 'Sage', version: '2.1', run_timestamp: '2026-02-01T00:00:00Z' },
      { agent_name: 'Guardian', version: '1.5', run_timestamp: '2026-02-01T00:00:00Z' },
    ],
    last_hitl_checkpoint: '2026-01-25T15:00:00Z',
    fit_score_algorithm: 'weighted-average-v1',
  };
}

// --- Test Suites ---

describe('Hash Stability Integration Tests', () => {
  describe('Timestamp Independence', () => {
    it('computeSourceEvidenceHash is unchanged when experiment dates change', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();

      // Mutate only date fields on experiment results
      evidence2.experiment_results = evidence2.experiment_results.map((exp) => ({
        ...exp,
        start_date: '2025-06-01',
        end_date: '2025-06-30',
      }));

      const hash1 = computeSourceEvidenceHash(evidence1);
      const hash2 = computeSourceEvidenceHash(evidence2);

      // Dates ARE part of the evidence content, so they SHOULD change the hash.
      // This test verifies the hash IS sensitive to date content in the data.
      expect(hash1).not.toBe(hash2);
    });

    it('computeSourceEvidenceHash is unchanged when HITL checkpoint timestamps change but sort order is preserved', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();

      // Change responded_at timestamps but keep the same sort order
      // (same checkpoint_type, responded_at is used as tiebreaker after type)
      evidence2.hitl_record.checkpoints = evidence2.hitl_record.checkpoints.map((cp) => ({
        ...cp,
        responded_at: cp.responded_at
          ? cp.responded_at.replace('2026-01', '2026-07')
          : cp.responded_at,
      }));

      const hash1 = computeSourceEvidenceHash(evidence1);
      const hash2 = computeSourceEvidenceHash(evidence2);

      // Timestamps are embedded in the data itself, so changing them changes the hash.
      // This confirms the hash captures the full content, including timestamps within the data.
      expect(hash1).not.toBe(hash2);
    });

    it('computeNarrativeHash ignores external timestamp columns (generated_at is not in PitchNarrativeContent)', () => {
      // PitchNarrativeContent does NOT contain generated_at or updated_at.
      // Those are separate DB columns on pitch_narratives, not part of the JSONB.
      // This test confirms that identical PitchNarrativeContent always hashes the same,
      // regardless of when it was generated.
      const content = makeNarrativeContent();

      const hashAtTimeA = computeNarrativeHash(content);
      // Simulating "different generation time" â€” the content is the same object
      const hashAtTimeB = computeNarrativeHash(content);

      expect(hashAtTimeA).toBe(hashAtTimeB);
    });

    it('computeIntegrityHash is unchanged when agent run_timestamps change but agent ordering is stable', () => {
      const evidence = makeValidationEvidence();
      const integrity1 = makeIntegrity();
      const integrity2 = {
        ...makeIntegrity(),
        agent_versions: makeIntegrity().agent_versions.map((av) => ({
          ...av,
          run_timestamp: '2025-01-01T00:00:00Z',
        })),
      };

      const hash1 = computeIntegrityHash(evidence, integrity1);
      const hash2 = computeIntegrityHash(evidence, integrity2);

      // run_timestamp is part of the integrity data, so changing it changes the hash.
      expect(hash1).not.toBe(hash2);
    });

    it('presentation_date in cover is part of content and affects narrative hash', () => {
      const content1 = makeNarrativeContent();
      const content2 = makeNarrativeContent({
        cover: {
          ...makeNarrativeContent().cover,
          presentation_date: '2025-01-01',
        },
      });

      const hash1 = computeNarrativeHash(content1);
      const hash2 = computeNarrativeHash(content2);

      expect(hash1).not.toBe(hash2);
    });
  });

  describe('Content Sensitivity', () => {
    it('computeNarrativeHash changes when thesis text changes', () => {
      const content1 = makeNarrativeContent();
      const content2 = makeNarrativeContent({
        overview: {
          ...makeNarrativeContent().overview,
          thesis: 'A completely different thesis.',
        },
      });

      expect(computeNarrativeHash(content1)).not.toBe(computeNarrativeHash(content2));
    });

    it('computeNarrativeHash changes when a single numeric score changes', () => {
      const content1 = makeNarrativeContent();
      const content2 = makeNarrativeContent({
        solution: {
          ...makeNarrativeContent().solution,
          fit_score: 0.79, // original is 0.78
        },
      });

      expect(computeNarrativeHash(content1)).not.toBe(computeNarrativeHash(content2));
    });

    it('computeSourceEvidenceHash changes when a single pain severity changes', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();
      evidence2.vpc.pains[0] = { ...evidence2.vpc.pains[0], severity: 0.91 }; // original is 0.9

      expect(computeSourceEvidenceHash(evidence1)).not.toBe(computeSourceEvidenceHash(evidence2));
    });

    it('computeSourceEvidenceHash changes when a competitor name changes', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();
      evidence2.competitor_map.competitors[0] = {
        ...evidence2.competitor_map.competitors[0],
        name: 'Different Corp',
      };

      expect(computeSourceEvidenceHash(evidence1)).not.toBe(computeSourceEvidenceHash(evidence2));
    });

    it('computeSourceEvidenceHash changes when an experiment outcome flips', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();
      evidence2.experiment_results[0] = {
        ...evidence2.experiment_results[0],
        outcome: 'invalidated',
      };

      expect(computeSourceEvidenceHash(evidence1)).not.toBe(computeSourceEvidenceHash(evidence2));
    });

    it('computeIntegrityHash changes when methodology_version changes', () => {
      const evidence = makeValidationEvidence();
      const integrity1 = makeIntegrity();
      const integrity2 = { ...makeIntegrity(), methodology_version: 'VPD-2.0' };

      expect(computeIntegrityHash(evidence, integrity1)).not.toBe(
        computeIntegrityHash(evidence, integrity2)
      );
    });

    it('computeIntegrityHash changes when fit_score_algorithm changes', () => {
      const evidence = makeValidationEvidence();
      const integrity1 = makeIntegrity();
      const integrity2 = { ...makeIntegrity(), fit_score_algorithm: 'bayesian-v2' };

      expect(computeIntegrityHash(evidence, integrity1)).not.toBe(
        computeIntegrityHash(evidence, integrity2)
      );
    });

    it('adding a new element to an array changes the hash', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();
      evidence2.vpc.customer_jobs = [...evidence2.vpc.customer_jobs, 'New job'];

      expect(computeSourceEvidenceHash(evidence1)).not.toBe(
        computeSourceEvidenceHash(evidence2)
      );
    });

    it('removing an element from an array changes the hash', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();
      evidence2.vpc.customer_jobs = [evidence2.vpc.customer_jobs[0]];

      expect(computeSourceEvidenceHash(evidence1)).not.toBe(
        computeSourceEvidenceHash(evidence2)
      );
    });
  });

  describe('Key Order Independence', () => {
    it('stableStringify produces identical output regardless of key insertion order', () => {
      const objA = { zebra: 1, apple: 2, mango: { z_inner: true, a_inner: false } };
      const objB = { apple: 2, mango: { a_inner: false, z_inner: true }, zebra: 1 };

      expect(stableStringify(objA)).toBe(stableStringify(objB));
    });

    it('computeNarrativeHash is stable when content is reconstructed with different key ordering', () => {
      const content = makeNarrativeContent();

      // Reconstruct via JSON round-trip (key order may differ in V8 edge cases)
      const serialized = JSON.stringify(content);
      const reconstructed = JSON.parse(serialized) as PitchNarrativeContent;

      expect(computeNarrativeHash(content)).toBe(computeNarrativeHash(reconstructed));
    });

    it('computeSourceEvidenceHash is stable when evidence is reconstructed from reverse-keyed object', () => {
      const evidence = makeValidationEvidence();

      // Deep clone and reverse top-level key order
      const reversed = Object.keys(evidence)
        .reverse()
        .reduce((acc, key) => {
          (acc as unknown as Record<string, unknown>)[key] = JSON.parse(
            JSON.stringify((evidence as unknown as Record<string, unknown>)[key])
          );
          return acc;
        }, {} as unknown as ValidationEvidence) as ValidationEvidence;

      expect(computeSourceEvidenceHash(evidence)).toBe(computeSourceEvidenceHash(reversed));
    });

    it('computeIntegrityHash is stable with reordered integrity keys', () => {
      const evidence = makeValidationEvidence();
      const integrity = makeIntegrity();

      // Reorder keys of the integrity object
      const reordered = {
        fit_score_algorithm: integrity.fit_score_algorithm,
        agent_versions: integrity.agent_versions,
        last_hitl_checkpoint: integrity.last_hitl_checkpoint,
        methodology_version: integrity.methodology_version,
      };

      expect(computeIntegrityHash(evidence, integrity)).toBe(
        computeIntegrityHash(evidence, reordered)
      );
    });

    it('deeply nested key reordering does not affect hash', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();

      // Reconstruct a nested object with reversed keys
      evidence2.vpc = {
        fit_assessment: evidence1.vpc.fit_assessment,
        products_services: [...evidence1.vpc.products_services],
        gain_creators: [...evidence1.vpc.gain_creators],
        pain_relievers: [...evidence1.vpc.pain_relievers],
        gains: [...evidence1.vpc.gains],
        pains: [...evidence1.vpc.pains],
        customer_jobs: [...evidence1.vpc.customer_jobs],
        customer_segment: evidence1.vpc.customer_segment,
      };

      expect(computeSourceEvidenceHash(evidence1)).toBe(computeSourceEvidenceHash(evidence2));
    });
  });

  describe('Array Reordering Canonicalization', () => {
    it('VPC customer_jobs in different order produce the same hash', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();
      evidence2.vpc.customer_jobs = ['Automate verification', 'Ensure data integrity']; // reversed

      expect(computeSourceEvidenceHash(evidence1)).toBe(computeSourceEvidenceHash(evidence2));
    });

    it('VPC pains in different order produce the same hash (sorted by description)', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();
      evidence2.vpc.pains = [
        { description: 'Key ordering bugs', severity: 0.7 },
        { description: 'Hash mismatches', severity: 0.9 },
      ]; // reversed

      expect(computeSourceEvidenceHash(evidence1)).toBe(computeSourceEvidenceHash(evidence2));
    });

    it('VPC gains in different order produce the same hash (sorted by description)', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();
      evidence2.vpc.gains = [
        { description: 'Cross-platform parity', importance: 0.8 },
        { description: 'Deterministic output', importance: 0.95 },
      ]; // reversed

      expect(computeSourceEvidenceHash(evidence1)).toBe(computeSourceEvidenceHash(evidence2));
    });

    it('BMC string arrays in different order produce the same hash', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();

      // Reverse all BMC string arrays
      evidence2.bmc = {
        ...evidence2.bmc,
        key_partners: ['OSS foundations', 'Cloud providers'],
        key_activities: ['Spec maintenance', 'SDK development'],
        key_resources: ['Spec documents', 'Engineering team'],
        value_propositions: ['Simplicity', 'Speed', 'Determinism'],
        customer_relationships: ['Community support', 'Developer docs'],
        channels: ['PyPI', 'npm'],
        customer_segments: ['Security teams', 'Platform teams'],
      };

      expect(computeSourceEvidenceHash(evidence1)).toBe(computeSourceEvidenceHash(evidence2));
    });

    it('competitors in different order produce the same hash (sorted by name)', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();
      evidence2.competitor_map.competitors = [
        evidence1.competitor_map.competitors[1],
        evidence1.competitor_map.competitors[0],
      ]; // swapped

      expect(computeSourceEvidenceHash(evidence1)).toBe(computeSourceEvidenceHash(evidence2));
    });

    it('experiment_results in different order produce the same hash (sorted by experiment_id)', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();
      evidence2.experiment_results = [
        evidence1.experiment_results[1],
        evidence1.experiment_results[0],
      ]; // swapped

      expect(computeSourceEvidenceHash(evidence1)).toBe(computeSourceEvidenceHash(evidence2));
    });

    it('HITL checkpoints in different order produce the same hash (sorted by type then time)', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();
      evidence2.hitl_record = {
        ...evidence2.hitl_record,
        checkpoints: [
          evidence1.hitl_record.checkpoints[1], // feasibility_review
          evidence1.hitl_record.checkpoints[0], // desirability_review
        ],
      };

      expect(computeSourceEvidenceHash(evidence1)).toBe(computeSourceEvidenceHash(evidence2));
    });

    it('customer_profile jobs_to_be_done in different order produce the same hash (sorted by job)', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();
      evidence2.customer_profile.jobs_to_be_done = [
        evidence1.customer_profile.jobs_to_be_done[1], // 'Detect tampering'
        evidence1.customer_profile.jobs_to_be_done[0], // 'Verify exports'
      ];

      expect(computeSourceEvidenceHash(evidence1)).toBe(computeSourceEvidenceHash(evidence2));
    });

    it('customer_profile pains in different order produce the same hash (sorted by pain)', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();
      evidence2.customer_profile.pains = [
        evidence1.customer_profile.pains[1], // 'Slow verification'
        evidence1.customer_profile.pains[0], // 'Non-deterministic JSON'
      ];

      expect(computeSourceEvidenceHash(evidence1)).toBe(computeSourceEvidenceHash(evidence2));
    });

    it('customer_profile gains in different order produce the same hash (sorted by gain)', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();
      evidence2.customer_profile.gains = [
        evidence1.customer_profile.gains[1], // 'Audit trail'
        evidence1.customer_profile.gains[0], // 'Instant verification'
      ];

      expect(computeSourceEvidenceHash(evidence1)).toBe(computeSourceEvidenceHash(evidence2));
    });

    it('behavioral_insights in different order produce the same hash (string array sort)', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();
      evidence2.customer_profile.behavioral_insights = [
        'Value reproducibility',
        'Prefer deterministic tools',
      ]; // reversed

      expect(computeSourceEvidenceHash(evidence1)).toBe(computeSourceEvidenceHash(evidence2));
    });

    it('integrity agent_versions in different order produce the same hash (sorted by agent_name)', () => {
      const evidence = makeValidationEvidence();
      const integrity1 = makeIntegrity(); // [Sage, Guardian]
      const integrity2 = {
        ...makeIntegrity(),
        agent_versions: [
          makeIntegrity().agent_versions[1], // Guardian
          makeIntegrity().agent_versions[0], // Sage
        ],
      };

      expect(computeIntegrityHash(evidence, integrity1)).toBe(
        computeIntegrityHash(evidence, integrity2)
      );
    });
  });

  describe('Null and Undefined Handling', () => {
    it('stableStringify treats null as "null" in output', () => {
      const result = stableStringify({ key: null });
      expect(result).toBe('{"key":null}');
    });

    it('stableStringify omits undefined keys (JSON.stringify behavior)', () => {
      const result = stableStringify({ present: 1, absent: undefined });
      expect(result).toBe('{"present":1}');
    });

    it('null and undefined in same position produce different stableStringify output', () => {
      const withNull = stableStringify({ value: null });
      const withUndefined = stableStringify({ value: undefined });

      // null is serialized as {"value":null}
      // undefined is dropped, producing {}
      expect(withNull).not.toBe(withUndefined);
    });

    it('null fields within arrays are preserved', () => {
      const result = stableStringify({ items: [null, 'a', null] });
      expect(result).toBe('{"items":[null,"a",null]}');
    });

    it('undefined inside arrays becomes null (JSON.stringify behavior)', () => {
      const result = stableStringify({ items: [undefined, 'a'] });
      // JSON.stringify converts array undefined to null
      expect(result).toBe('{"items":[null,"a"]}');
    });

    it('optional fields being absent does not affect hash when absent on both sides', () => {
      const content1 = makeNarrativeContent();
      const content2 = makeNarrativeContent();

      // Both lack optional fields (logo_url, hero_image_url, etc.)
      expect(computeNarrativeHash(content1)).toBe(computeNarrativeHash(content2));
    });

    it('adding an optional field that was absent changes the hash', () => {
      const content1 = makeNarrativeContent();
      const content2 = makeNarrativeContent({
        cover: {
          ...makeNarrativeContent().cover,
          logo_url: 'https://example.com/logo.png',
        },
      });

      expect(computeNarrativeHash(content1)).not.toBe(computeNarrativeHash(content2));
    });

    it('null responded_at on HITL checkpoint is handled consistently', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();

      // Remove responded_at from one checkpoint (make it pending-like)
      evidence1.hitl_record.checkpoints = [
        {
          checkpoint_id: 'cp-null-test',
          checkpoint_type: 'alpha_review',
          triggered_at: '2026-02-01T00:00:00Z',
          approval_status: 'pending',
        },
      ];
      evidence2.hitl_record.checkpoints = [
        {
          checkpoint_id: 'cp-null-test',
          checkpoint_type: 'alpha_review',
          triggered_at: '2026-02-01T00:00:00Z',
          approval_status: 'pending',
        },
      ];

      expect(computeSourceEvidenceHash(evidence1)).toBe(computeSourceEvidenceHash(evidence2));
    });
  });

  describe('Cross-Implementation Output Format', () => {
    it('computeNarrativeHash returns exactly 64 hex characters', () => {
      const hash = computeNarrativeHash(makeNarrativeContent());
      expect(hash).toHaveLength(64);
      expect(hash).toMatch(/^[0-9a-f]{64}$/);
    });

    it('computeSourceEvidenceHash returns exactly 64 hex characters', () => {
      const hash = computeSourceEvidenceHash(makeValidationEvidence());
      expect(hash).toHaveLength(64);
      expect(hash).toMatch(/^[0-9a-f]{64}$/);
    });

    it('computeIntegrityHash returns exactly 64 hex characters', () => {
      const hash = computeIntegrityHash(makeValidationEvidence(), makeIntegrity());
      expect(hash).toHaveLength(64);
      expect(hash).toMatch(/^[0-9a-f]{64}$/);
    });

    it('hash output uses only lowercase hex digits (no uppercase)', () => {
      const hashes = [
        computeNarrativeHash(makeNarrativeContent()),
        computeSourceEvidenceHash(makeValidationEvidence()),
        computeIntegrityHash(makeValidationEvidence(), makeIntegrity()),
      ];

      for (const hash of hashes) {
        expect(hash).toBe(hash.toLowerCase());
        expect(hash).not.toMatch(/[A-F]/);
      }
    });

    it('all three hash functions produce distinct outputs for their respective inputs', () => {
      const narrativeHash = computeNarrativeHash(makeNarrativeContent());
      const evidenceHash = computeSourceEvidenceHash(makeValidationEvidence());
      const integrityHash = computeIntegrityHash(makeValidationEvidence(), makeIntegrity());

      // All three should be different (different input domains)
      const uniqueHashes = new Set([narrativeHash, evidenceHash, integrityHash]);
      expect(uniqueHashes.size).toBe(3);
    });

    it('hash is deterministic across multiple invocations', () => {
      const content = makeNarrativeContent();
      const evidence = makeValidationEvidence();
      const integrity = makeIntegrity();

      const results = Array.from({ length: 10 }, () => ({
        narrative: computeNarrativeHash(content),
        evidence: computeSourceEvidenceHash(evidence),
        integrity: computeIntegrityHash(evidence, integrity),
      }));

      // All 10 runs should produce identical hashes
      for (let i = 1; i < results.length; i++) {
        expect(results[i].narrative).toBe(results[0].narrative);
        expect(results[i].evidence).toBe(results[0].evidence);
        expect(results[i].integrity).toBe(results[0].integrity);
      }
    });
  });

  describe('Canonicalization Pipeline Integration', () => {
    it('canonicalizeEvidence followed by stableStringify is fully deterministic', () => {
      const evidence = makeValidationEvidence();

      const run1 = stableStringify(canonicalizeEvidence(evidence));
      const run2 = stableStringify(canonicalizeEvidence(evidence));

      expect(run1).toBe(run2);
    });

    it('canonicalizeEvidence does not mutate the original evidence', () => {
      const evidence = makeValidationEvidence();
      const originalJobsOrder = [...evidence.vpc.customer_jobs];

      canonicalizeEvidence(evidence);

      expect(evidence.vpc.customer_jobs).toEqual(originalJobsOrder);
    });

    it('full pipeline produces same hash for maximally shuffled evidence', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();

      // Shuffle every sortable array in evidence2
      evidence2.vpc.customer_jobs = [...evidence1.vpc.customer_jobs].reverse();
      evidence2.vpc.pains = [...evidence1.vpc.pains].reverse();
      evidence2.vpc.gains = [...evidence1.vpc.gains].reverse();
      evidence2.vpc.pain_relievers = [...evidence1.vpc.pain_relievers].reverse();
      evidence2.vpc.gain_creators = [...evidence1.vpc.gain_creators].reverse();
      evidence2.vpc.products_services = [...evidence1.vpc.products_services].reverse();
      evidence2.customer_profile.jobs_to_be_done = [...evidence1.customer_profile.jobs_to_be_done].reverse();
      evidence2.customer_profile.pains = [...evidence1.customer_profile.pains].reverse();
      evidence2.customer_profile.gains = [...evidence1.customer_profile.gains].reverse();
      evidence2.customer_profile.behavioral_insights = [...evidence1.customer_profile.behavioral_insights].reverse();
      evidence2.competitor_map.competitors = [...evidence1.competitor_map.competitors].reverse();
      evidence2.bmc.key_partners = [...evidence1.bmc.key_partners].reverse();
      evidence2.bmc.key_activities = [...evidence1.bmc.key_activities].reverse();
      evidence2.bmc.key_resources = [...evidence1.bmc.key_resources].reverse();
      evidence2.bmc.value_propositions = [...evidence1.bmc.value_propositions].reverse();
      evidence2.bmc.customer_relationships = [...evidence1.bmc.customer_relationships].reverse();
      evidence2.bmc.channels = [...evidence1.bmc.channels].reverse();
      evidence2.bmc.customer_segments = [...evidence1.bmc.customer_segments].reverse();
      evidence2.experiment_results = [...evidence1.experiment_results].reverse();
      evidence2.hitl_record = {
        ...evidence2.hitl_record,
        checkpoints: [...evidence1.hitl_record.checkpoints].reverse(),
      };

      expect(computeSourceEvidenceHash(evidence1)).toBe(computeSourceEvidenceHash(evidence2));
    });

    it('full integrity pipeline produces same hash for shuffled evidence AND shuffled integrity', () => {
      const evidence1 = makeValidationEvidence();
      const evidence2 = makeValidationEvidence();

      // Shuffle a subset of evidence arrays
      evidence2.vpc.customer_jobs = [...evidence1.vpc.customer_jobs].reverse();
      evidence2.competitor_map.competitors = [...evidence1.competitor_map.competitors].reverse();
      evidence2.experiment_results = [...evidence1.experiment_results].reverse();

      const integrity1 = makeIntegrity();
      const integrity2 = {
        ...makeIntegrity(),
        agent_versions: [...makeIntegrity().agent_versions].reverse(),
      };

      expect(computeIntegrityHash(evidence1, integrity1)).toBe(
        computeIntegrityHash(evidence2, integrity2)
      );
    });
  });
});
